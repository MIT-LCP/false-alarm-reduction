{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime                     import datetime\n",
    "import numpy                      as np\n",
    "import regular_activity           as regular\n",
    "import load_annotations           as annotate\n",
    "import invalid_sample_detection   as invalid\n",
    "import parameters\n",
    "import wfdb\n",
    "import os\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "data_path = 'sample_data/challenge_training_data/'\n",
    "ann_path = 'sample_data/challenge_training_multiann/'\n",
    "ecg_ann_type = 'gqrs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix for all samples given sample name/directory\n",
    "def generate_confusion_matrix_sample(data_path, ann_path, ecg_ann_type, should_check_invalids=True,\n",
    "                                         should_check_rr=True): \n",
    "    confusion_matrix = {\n",
    "        \"TP\": [],\n",
    "        \"FP\": [],\n",
    "        \"FN\": [],\n",
    "        \"TN\": []\n",
    "    }\n",
    "    \n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(parameters.HEADER_EXTENSION):\n",
    "            sample_name = filename.rstrip(parameters.HEADER_EXTENSION)            \n",
    "            \n",
    "            sig, fields = wfdb.rdsamp(data_path + sample_name)\n",
    "            alarm_type, is_true_alarm = regular.check_gold_standard_classification(fields)\n",
    "            \n",
    "            if invalid.contains_nan(sig): \n",
    "                print sample_name\n",
    "            \n",
    "            is_regular = regular.is_sample_regular(data_path, ann_path, sample_name, ecg_ann_type,\n",
    "                                                   start=None, end=None, should_check_invalids=should_check_invalids, should_check_rr=should_check_rr)\n",
    "            \n",
    "            # Classified as a true alarm if no regular activity \n",
    "            classified_true_alarm = not is_regular\n",
    "            \n",
    "            if is_true_alarm and classified_true_alarm: \n",
    "                confusion_matrix[\"TP\"].append(sample_name)\n",
    "                \n",
    "            elif is_true_alarm and not classified_true_alarm: \n",
    "                confusion_matrix[\"FN\"].append(sample_name)\n",
    "                print \"FALSE NEGATIVE: \", filename\n",
    "                \n",
    "            elif not is_true_alarm and classified_true_alarm: \n",
    "                confusion_matrix[\"FP\"].append(sample_name)\n",
    "                \n",
    "            else: \n",
    "                confusion_matrix[\"TN\"].append(sample_name)\n",
    "                \n",
    "    counts = { key : len(confusion_matrix[key]) for key in confusion_matrix.keys() }\n",
    "                \n",
    "    return counts, confusion_matrix\n",
    "\n",
    "# generate_confusion_matrix_sample(data_path, ann_path, 'jqrs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with saving intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_rr_file(data_path, ann_path, ecg_ann_type): \n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(parameters.HEADER_EXTENSION):\n",
    "            sample_name = filename.rstrip(parameters.HEADER_EXTENSION)            \n",
    "            \n",
    "            sig, fields = wfdb.rdsamp(data_path + sample_name)\n",
    "            start, end, alarm_duration = invalid.get_start_and_end(fields)\n",
    "            channels = fields['signame']\n",
    "            num_channels = len(channels)\n",
    "            ann_filename = \"rr_\" + ecg_ann_type + \".json\"\n",
    "            \n",
    "            rr_dict = {}\n",
    "            for channel_index in range(num_channels): \n",
    "                channel = fields['signame'][channel_index]\n",
    "                # Ignore respiratory channel\n",
    "                if channel == \"Resp\": \n",
    "                    continue\n",
    "\n",
    "                rr_intervals, duration = annotate.get_channel_rr_intervals(ann_path, sample_name, channel_index, \n",
    "                                                                           fields, ecg_ann_type)\n",
    "                rr_dict[channel] = rr_intervals.tolist()\n",
    "\n",
    "def split_data(data1, data2): \n",
    "    only_in_data1 = set([])\n",
    "    only_in_data2 = set([])\n",
    "    \n",
    "    for element in data1: \n",
    "        if element not in data2: \n",
    "            only_in_data1.add(element)\n",
    "            \n",
    "    for element in data2: \n",
    "        if element not in data1: \n",
    "            only_in_data2.add(element)\n",
    "            \n",
    "    return only_in_data1, only_in_data2\n",
    "\n",
    "def read_rr_file(ann_path, ecg_ann_type):\n",
    "    all_rr_dict = {}\n",
    "    sample_name = \"\"\n",
    "    ann_filename = \"rr_\" + ecg_ann_type + \".json\"\n",
    "\n",
    "    with open(os.path.join(ann_path, ann_filename), 'r') as f: \n",
    "        for line in f: \n",
    "            try:\n",
    "                rr_intervals = json.loads(line)\n",
    "                all_rr_dict[sample_name] = rr_intervals\n",
    "            except Exception as e: \n",
    "                sample_name = line.strip()\n",
    "                \n",
    "    return all_rr_dict\n",
    "\n",
    "    \n",
    "def write_invalids_file(data_path): \n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(parameters.HEADER_EXTENSION):\n",
    "            sample_name = filename.rstrip(parameters.HEADER_EXTENSION)            \n",
    "            \n",
    "            sig, fields = wfdb.rdsamp(data_path + sample_name)\n",
    "            start, end, alarm_duration = invalid.get_start_and_end(fields)\n",
    "            \n",
    "            invalids = invalid.calculate_invalids(data_path + sample_name, start, end)\n",
    "            invalids_jsonifiable = { key : invalids[key].tolist() for key in invalids.keys() }\n",
    "            \n",
    "            with open(os.path.join(data_path, \"invalids.json\"), \"a\") as f: \n",
    "                f.write(\"\\n\" + sample_name + \"\\n\")\n",
    "                json.dump(invalids_jsonifiable, f)\n",
    "                \n",
    "def read_invalids_file(data_path):\n",
    "    all_invalids_dict = {}\n",
    "    sample_name = \"\"\n",
    "    \n",
    "    with open(os.path.join(data_path, \"invalids.json\"), 'r') as f: \n",
    "        for line in f: \n",
    "            try:\n",
    "                invalids_dict = json.loads(line)\n",
    "                all_invalids_dict[sample_name] = invalids_dict\n",
    "            except Exception as e: \n",
    "                sample_name = line.strip()\n",
    "                \n",
    "    return all_invalids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix for all samples given intermediate data\n",
    "def generate_confusion_matrix_intermediate(data_path, ann_path, ecg_ann_type, \n",
    "                                           should_check_invalids=True, should_check_rr=True): \n",
    "    confusion_matrix = {\n",
    "        \"TP\": [],\n",
    "        \"FP\": [],\n",
    "        \"FN\": [],\n",
    "        \"TN\": []\n",
    "    }\n",
    "    \n",
    "    all_invalids_dict = read_invalids_file(data_path)\n",
    "    all_rr_dict = read_rr_file(ann_path, ecg_ann_type)\n",
    "    \n",
    "    for sample_name in all_invalids_dict.keys(): \n",
    "        invalids_dict = all_invalids_dict[sample_name]\n",
    "        \n",
    "        rr_intervals_dict = {}\n",
    "        if sample_name in all_rr_dict.keys(): \n",
    "            rr_intervals_dict = all_rr_dict[sample_name]\n",
    "        \n",
    "        sig, fields = wfdb.rdsamp(data_path + sample_name)\n",
    "        alarm_type, is_true_alarm = regular.check_gold_standard_classification(fields)\n",
    "        start, end, alarm_duration = invalid.get_start_and_end(fields)        \n",
    "\n",
    "        is_regular = regular.is_rr_invalids_regular(rr_intervals_dict, invalids_dict, alarm_duration,\n",
    "                                                    should_check_invalids, should_check_rr)\n",
    "            \n",
    "        # Classified as a true alarm if no regular activity \n",
    "        classified_true_alarm = not is_regular\n",
    "\n",
    "        if is_true_alarm and classified_true_alarm: \n",
    "            confusion_matrix[\"TP\"].append(sample_name)\n",
    "\n",
    "        elif is_true_alarm and not classified_true_alarm: \n",
    "            confusion_matrix[\"FN\"].append(sample_name)\n",
    "            print \"FALSE NEGATIVE: \", sample_name\n",
    "\n",
    "        elif not is_true_alarm and classified_true_alarm: \n",
    "            confusion_matrix[\"FP\"].append(sample_name)\n",
    "\n",
    "        else: \n",
    "            confusion_matrix[\"TN\"].append(sample_name)\n",
    "            \n",
    "    counts = { key : len(confusion_matrix[key]) for key in confusion_matrix.keys() }\n",
    "                \n",
    "    return counts, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate invalids, jqrs, and gqrs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_path = \"sample_data/challenge_training_data/\"\n",
    "# ann_path = \"sample_data/challenge_training_multiann/\"\n",
    "\n",
    "# with open(os.path.join(data_path, \"invalids.json\"), 'w'): \n",
    "#     pass\n",
    "# with open(os.path.join(ann_path, \"rr_jqrs.json\"), 'w'): \n",
    "#     pass\n",
    "# with open(os.path.join(ann_path, \"rr_gqrs.json\"), 'w'): \n",
    "#     pass\n",
    "\n",
    "\n",
    "# print \"Writing jqrs file...\"\n",
    "# write_rr_file(data_path, ann_path, 'jqrs')\n",
    "# print \"Writing gqrs file...\"\n",
    "# write_rr_file(data_path, ann_path, 'gqrs')\n",
    "# print \"Writing invalids file...\"\n",
    "# write_invalids_file(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_sensitivity(counts): \n",
    "    tp = counts[\"TP\"]\n",
    "    fn = counts[\"FN\"]\n",
    "    return tp / float(tp + fn)\n",
    "    \n",
    "def calc_specificity(counts): \n",
    "    tn = counts[\"TN\"]\n",
    "    fp = counts[\"FP\"]\n",
    "    \n",
    "    return tn / float(tn + fp)\n",
    "\n",
    "def calc_ppv(counts): \n",
    "    tp = counts[\"TP\"]\n",
    "    fp = counts[\"FP\"]\n",
    "    return tp / float(tp + fp)\n",
    "\n",
    "def calc_f1(counts): \n",
    "    sensitivity = calc_sensitivity(counts)\n",
    "    ppv = calc_ppv(counts)\n",
    "    \n",
    "    return 2 * sensitivity * ppv / float(sensitivity + ppv)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(counts): \n",
    "    sensitivity = calc_sensitivity(counts)\n",
    "    specificity = calc_specificity(counts)\n",
    "    ppv = calc_ppv(counts)\n",
    "    f1 = calc_f1(counts)\n",
    "    score = float(counts[\"TP\"] + counts[\"TN\"])/(counts[\"TP\"] + counts[\"FP\"] + counts[\"TN\"] + counts[\"FN\"] * 5)\n",
    "\n",
    "    print \"counts: \", counts\n",
    "    print \"sensitivity: \", sensitivity\n",
    "    print \"specificity: \", specificity\n",
    "    print \"ppv: \", ppv\n",
    "    print \"f1: \", f1\n",
    "    print \"score: \", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_confusion_matrix_intermediate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0fb153406389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     generate_confusion_matrix_sample(data_path, ann_path, 'jqrs')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcounts_jqrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix_jqrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_confusion_matrix_intermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mann_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jqrs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#     counts_rr, confusion_matrix_rr = generate_confusion_matrix_intermediate(data_path, ann_path, 'jqrs',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#                                                                                 False, # should_check_invalids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_confusion_matrix_intermediate' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    data_path = 'sample_data/challenge_training_data/'\n",
    "    ann_path = 'sample_data/challenge_training_multiann/'\n",
    "\n",
    "#     generate_confusion_matrix_sample(data_path, ann_path, 'jqrs')\n",
    "    \n",
    "    counts_jqrs, confusion_matrix_jqrs = generate_confusion_matrix_intermediate(data_path, ann_path, 'jqrs')\n",
    "#     counts_rr, confusion_matrix_rr = generate_confusion_matrix_intermediate(data_path, ann_path, 'jqrs', \n",
    "#                                                                                 False, # should_check_invalids\n",
    "#                                                                                 True # should_check_rr\n",
    "#                                                                                 )\n",
    "\n",
    "    print_stats(counts_jqrs)\n",
    "#     print_stats(counts_rr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
